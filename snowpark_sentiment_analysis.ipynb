{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NLTK SENTIMENT SCORING WITH SNOWPARK**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ›‘ IMPORTANT! Snowpark requires Python 3.8!\n",
    "#### **Please download this Notebook as a Jupyter file and run it local on a Python 3.8 environment.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Install Snowpark & NLTK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!pip install snowflake-snowpark-python\n",
    "!pip install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a NLTK Sentiment Python Function (No Snowpark Yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ---> Download the language model & create an object reference\n",
    "nltk.download('vader_lexicon')\n",
    "my_ntlk = SentimentIntensityAnalyzer()\n",
    "\n",
    "# -- Build a Python function using NLTK library to return a sentiment score based on a text input\n",
    "def GetSentiment(review: str) -> float:\n",
    "    return my_ntlk.polarity_scores(review)[\"compound\"]\n",
    "\n",
    "# Test the Python function on local compute\n",
    "\n",
    "GoodScore = GetSentiment('This was good movie')\n",
    "BadScore = GetSentiment('This is the worst book')\n",
    "\n",
    "print(f'Good score = {GoodScore}  &  BadScore = {BadScore}        \\n\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Get Your Snowflake Account Locator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify your Snowflake Account & User credentials in this section**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this SQL on your Snowflake account to get the account name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT SPLIT_PART(VALUE:host::string , '.',1) AS SNOWFLAKE_ACCOUNT_LOCATOR\n",
    "from table(flatten(parse_json(SYSTEM$ALLOWLIST())))\n",
    "where VALUE:type::string = 'SNOWFLAKE_DEPLOYMENT_REGIONLESS'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Import libraries, Connect to Snowflake & Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "from snowflake.snowpark import Session, DataFrame\n",
    "from snowflake.snowpark.functions import udf, col\n",
    "import snowflake.snowpark.functions as f\n",
    "from snowflake.snowpark.functions import udf\n",
    "from snowflake.snowpark.types import IntegerType\n",
    "from snowflake.snowpark.functions import call_udf\n",
    "\n",
    "\n",
    "# --> EDIT THIS WITH YOUR ACCOUNT & CREDS  \n",
    "CONNECTION_PARAMETERS= {\n",
    "    'account': '<Snowflake_Account_Locator>',\n",
    "    'user': 'SomeUser',\n",
    "    'password': 'Not4u2Know',\n",
    "    'role': 'SYSADMIN'\n",
    "}\n",
    "\n",
    "print(\"Connecting to Snowflake.....\\n\")\n",
    "\n",
    "session = Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "print(\"Connected Successfully!..\\n\\n\")\n",
    "\n",
    "# Create the necessary Compute warehouse, Database, Schema & Stage to link to S3 Sample data\n",
    "print(\"Creating Snowflake objects (DB, Schema, FileFormat, Stage & Warehouse/Compute \")\n",
    "\n",
    "Warehouse_Name = 'MY_DEMO_WH'\n",
    "Warehouse_Size = 'LARGE'\n",
    "DB_name = 'DEMO_SNOWPARK'\n",
    "Schema_Name = 'Public'\n",
    "\n",
    "sql_cmd = f\"CREATE OR REPLACE WAREHOUSE {Warehouse_Name} WAREHOUSE_SIZE = 'MEDIUM' AUTO_SUSPEND = 10 \"\n",
    "session.sql(sql_cmd).collect() \n",
    "\n",
    "session.use_warehouse(Warehouse_Name)\n",
    "\n",
    "sql_cmd = \"CREATE DATABASE IF NOT EXISTS {}\".format(DB_name)\n",
    "session.sql(sql_cmd).collect() \n",
    "\n",
    "session.use_database(DB_name)\n",
    "session.use_schema(Schema_Name)\n",
    "\n",
    "session.sql(\"CREATE OR REPLACE STAGE my_s3_stage URL = 's3://amazon-reviews-pds/parquet/' \").collect()\n",
    "session.sql('CREATE OR REPLACE STAGE PythonUDF_Stage').collect()\n",
    "print('Demo Enviroment with MEDIUM Compute is Created!')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Ingest 6M Amazon reviews from parquet on S3 & Upload Python Function to Snowflake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below test is for Apperal Category with 6M reviews. Fullset in this S3 bucket contains 160M product reviews\n",
    "\n",
    "If you want to test it with a full set of 160M reviews, **enable lines 5 & 6**(removes references to the single file & Scales up the warehouse to 3XL), you should be able to score all **160M reviews in about 4 mins & 45 secs.** in which it scale down to XS once done.\n",
    "\n",
    "ðŸ›‘ **CAUTION. Make sure scale down command executes as 3XL can consume credits fast if left running.** If not, you can manually do it from UI or via SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read data directly in to Snowpark dataframe from a parquet file (Schema Detection for column names)\n",
    "location = '@my_s3_stage/product_category=Apparel/'\n",
    "\n",
    "session.sql(f'list {location}').collect()\n",
    "\n",
    "# - Get the total file size of Source \n",
    "s3_size =  session.sql('select count(1) as File_Count, sum($2/1024/1024/1024)::number(10,2) as Parquet_Size_In_GB FROM table(result_scan(last_query_id()))')\n",
    "s3_size.show()\n",
    "\n",
    "print(\"Importing Reviews data from S3....\")\n",
    "start_time = time.time()\n",
    "\n",
    "# - Ingest data from S3 Parquet files\n",
    "df_reviews = session.read.option(\"compression\", \"snappy\").parquet(location)\n",
    "\n",
    "# - UPPERCASE all column names to fix any mixed column names\n",
    "for Mycol in df_reviews.columns:\n",
    "    df_reviews = df_reviews.withColumnRenamed(Mycol, Mycol.upper())\n",
    "\n",
    "# - Store the ingested data into Snowflake Table\n",
    "df_reviews.write.mode(\"overwrite\").saveAsTable(\"AMAZON_REVIEWS\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds to Ingest 6M rows into a Table --- \\n\" % int(end_time - start_time))\n",
    "\n",
    "# - Get the size of the Snowflake table after the ingestion \n",
    "table_size = session.sql(\"SELECT TABLE_CATALOG, TABLE_NAME,(BYTES / 1024/1024/1024)::number(10,2) AS Snowflake_Size_In_GB\\\n",
    "                         FROM DEMO_SNOWPARK.INFORMATION_SCHEMA.TABLES WHERE TABLE_CATALOG = 'DEMO_SNOWPARK' AND TABLE_NAME = 'AMAZON_REVIEWS' \") \n",
    "table_size.show()\n",
    "\n",
    "\n",
    "# - Reference the Snowflake Table\n",
    "df_reviews = session.table(\"AMAZON_REVIEWS\")\n",
    "\n",
    "\n",
    "print(\"Reviews Ingested: \", df_reviews.count() )\n",
    "\n",
    "# - Filter the reviews with over 50 characters\n",
    "df_reviews = df_reviews.filter(f.length( col(\"review_body\")) > 50 )\n",
    "\n",
    "print(\"Reviews to Score: \", df_reviews.count())\n",
    "df_reviews.show()\n",
    "\n",
    "\n",
    "# Tell Snowpark to add NLTK library from Anaconda (You can also manually upload any compatible 3rd party library if it is not available via Anaconda)\n",
    "session.add_packages(\"nltk\")\n",
    "\n",
    "# 3. Upload & register the Python function + NLTK libray to Snowflake\n",
    "\n",
    "print(\"\\n\\nUploading & Registering Python Function as a Snowflake UDF....\")\n",
    "GetSentiment_udf = session.udf.register(GetSentiment, name=\"GetSentiment\", is_permanent=True, replace=True,\n",
    "                                    stage_location=\"@PythonUDF_Stage/PythonUDFS\")\n",
    "\n",
    "print(\"Upload Successful!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Score 4.5M Reviews & Write Results to a new Snowflake Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start the Sentiment Analysis & Write results to a new table called AMAZON_REVIEWS_RESULTS....\")\n",
    "\n",
    "print(f\"Scaling Up the warehouse to {Warehouse_Size}..\\n\")\n",
    "sql_cmd = f\"ALTER WAREHOUSE {Warehouse_Name} SET WAREHOUSE_SIZE = '{Warehouse_Size}' WAIT_FOR_COMPLETION = TRUE\"\n",
    "session.sql(sql_cmd).collect() \n",
    "\n",
    "print(\"Starting Sentiment Scoring Process...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 5. Start the Sentiment Scoring Process by running the Python function in Snowflake\n",
    "#final_df = df_reviews.group_by(col(\"review_date\"),col(\"review_id\"), col(\"product_id\"),  col(\"review_body\")).agg(call_udf(\"GetSentiment\", col(\"review_body\")).alias(\"score\"))\n",
    "final_df = df_reviews.group_by(col(\"review_date\"),col(\"review_id\"), col(\"product_id\"),  col(\"review_body\")).agg( GetSentiment_udf(col(\"review_body\")).alias(\"score\") )\n",
    "\n",
    "# 6. Write the resulting dataset to new table\n",
    "final_df.write.mode(\"overwrite\").saveAsTable(\"AMAZON_REVIEWS_RESULTS\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"--- %s seconds to Sentiment Analysis & Write the Results to Table --- \\n\" % int(end_time - start_time))\n",
    "\n",
    "final_df = session.table(\"AMAZON_REVIEWS_RESULTS\")\n",
    "\n",
    "print(\"Reviews Scored: \", final_df.count())\n",
    "\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "# ---DONE !!!\n",
    "\n",
    "# 7 - SCALE DOWN COMPUTE TO 1 NODE\n",
    "print(\"Reducing the warehouse to XS..\\n\")\n",
    "sql_cmd = \"ALTER WAREHOUSE {} SET WAREHOUSE_SIZE = 'XSMALL'\".format(Warehouse_Name)\n",
    "session.sql(sql_cmd).collect()  \n",
    "\n",
    "print(\"Completed!...\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](snowparkimage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sql_cmd = \"ALTER WAREHOUSE {} SET WAREHOUSE_SIZE = 'XSMALL'\".format(Warehouse_Name)\n",
    "session.sql(sql_cmd).collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
